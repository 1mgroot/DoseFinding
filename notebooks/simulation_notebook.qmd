---
title: "Bayesian Dose-Finding Trial Simulation"
format: html
editor: visual
---

## Introduction

This document provides an interactive way to run the Bayesian dose-finding trial simulation. You can modify the simulation parameters in the "Configuration" section and then run the code chunks to see the results.

## Setup

This chunk loads the necessary libraries and source files.

```{r setup}
library(knitr)
library(ggplot2)

# Use absolute paths
project_root <- "/Users/jz/Development/DoseFinding"
cat("Project root:", project_root, "\n")

# Source core files
source(file.path(project_root, "src/core/config.R"))
source(file.path(project_root, "src/utils/helpers.R"))
source(file.path(project_root, "src/utils/plotting_extensions.R"))
source(file.path(project_root, "src/core/simulate_data.R"))
source(file.path(project_root, "src/core/model_utils.R"))
source(file.path(project_root, "src/decision/dose_decision.R"))
source(file.path(project_root, "src/core/main.R"))
```

## Configuration

Modify the simulation parameters in this section. I have adjusted these values to create a more dynamic scenario where multiple doses are likely to be admissible in the early stages.

**Calibration Note**: The `c_poc` parameter has been calibrated using null/flat scenarios to control Type I error at approximately 10%. The optimal value of `c_poc = 0.95` achieves a 7.8% false positive rate. For stricter Type I error control matching the calibration conditions, consider using the threshold configuration from `poc_calibration_notebook.qmd` (phi_T=0.3, c_T=0.3, phi_E=0.25, c_E=0.3).

```{r config}
# Trial configuration
# Note: These parameters allow for interactive exploration with different scenarios.
# For calibrated Type I error control (~10%), use the stricter thresholds from 
# poc_calibration_notebook.qmd: phi_T=0.3, c_T=0.3, phi_E=0.25, c_E=0.3, phi_I=0.2, c_I=0.2

trial_config <- list(
  dose_levels = c(1, 2, 3, 4, 5),
  n_stages = 5,
  cohort_size = 15,
  phi_T = 0.35, # Toxicity threshold (relaxed for exploration)
  c_T = 0.5,   
  phi_E = 0.1, # Efficacy threshold (relaxed for exploration)
  c_E = 0.5,   
  phi_I = 0.20, # Immune response threshold
  c_I = 0.5,   
  # PoC parameters (calibrated value from null/flat scenario testing)
  c_poc = 0.95,  # Calibrated to achieve ~7.8% Type I error (target â‰¤10%)
  delta_poc = 0.8,  # Threshold for PoC pairwise comparison
  # Early termination parameters
  enable_early_termination = TRUE,
  log_early_termination = TRUE
)
#cohort size may vary on diff stage (future work)

# Data simulation parameters (designed for a more interesting trial)
p_YI <- c(0.10, 0.30, 0.50, 0.60, 0.70) # Immune response probability

p_YT_given_I <- matrix(c(
  # I=0 (No Immune Response)
  0.05, 0.10, 0.12, 0.18, 0.25,
  # I=1 (Immune Response)
  0.08, 0.12, 0.15, 0.25, 0.35
), nrow = 5, ncol = 2)

p_YE_given_I <- matrix(c(
  # I=0 (No Immune Response)
  0.10, 0.20, 0.35, 0.45, 0.50, 
  # I=1 (Immune Response)
  0.30, 0.50, 0.70, 0.80, 0.75  
), nrow = 5, ncol = 2)

rho0 <- 1.5
rho1 <- 2

# Utility table
# Rows: Efficacy (0, 1)
# Columns: Toxicity (0, 1)
# Slices: Immune Response (0, 1)
utility_table <- array(0, dim = c(2, 2, 2), dimnames = list(
  E = c(0, 1),
  T = c(0, 1),
  I = c(0, 1)
))

utility_table[1, 1, 1] <- 0   # E=0, T=0, I=0
utility_table[2, 1, 1] <- 80  # E=1, T=0, I=0
utility_table[1, 2, 1] <- 0   # E=0, T=1, I=0
utility_table[2, 2, 1] <- 30  # E=1, T=1, I=0

utility_table[1, 1, 2] <- 10  # E=0, T=0, I=1
utility_table[2, 1, 2] <- 100 # E=1, T=0, I=1
utility_table[1, 2, 2] <- 0   # E=0, T=1, I=1
utility_table[2, 2, 2] <- 40  # E=1, T=1, I=1

trial_config$utility_table <- utility_table
```

## Simulation

This chunk runs the multi-stage trial simulation by calling the `run_trial_simulation` function. The simulation follows the workflow specified in TRIAL_DESIGN.md Section 7.1:

1.  **Stage 1**: Equal randomization to all dose levels
2.  **Interim Analysis**: Update admissible set based on posterior probabilities
3.  **Early Termination Check**: Terminate if admissible set is empty (checked after interim analysis, before adaptive randomization)
4.  **Adaptive Randomization**: Allocate patients based on utility scores (Stages 2+ only, only if trial continues)
5.  **Final Selection**: Choose OD with highest utility from admissible set + PoC validation

```{r simulation}
results <- run_trial_simulation(trial_config, p_YI, p_YT_given_I, p_YE_given_I, rho0, rho1)
```

## Results

This chunk prints the final optimal dose and displays the plots.

```{r results}
# Print final results
cat("
--- Final Results ---
")

if (results$terminated_early) {
  cat("Trial terminated early at stage:", results$termination_stage, "
")
  cat("Reason:", results$termination_reason, "
")
  cat("No Optimal Dose selected
")
} else {
  cat("Final OD:", results$final_od, "
")
  cat("Final utility:", round(results$final_utility, 2), "
")
  cat("PoC validated:", results$poc_validated, "
")
  cat("PoC probability:", round(results$poc_probability, 3), "
")
  cat("Selection reason:", results$selection_reason, "
")
}

# Plot final results and save them with modern styling
plot_posterior_summary(results$posterior_summaries$imm, title = "Immune Response vs Dose (PAVA Adjusted)", file_path = "results/plots/immune_response_refactored.png", style = "modern")
plot_posterior_summary(results$posterior_summaries$tox, title = "Toxicity Rate by Dose and Immune Status", group_col = "Y_I", file_path = "results/plots/toxicity_refactored.png", style = "modern")
plot_posterior_summary(results$posterior_summaries$eff, title = "Efficacy Rate by Dose and Immune Status", group_col = "Y_I", file_path = "results/plots/efficacy_refactored.png", style = "modern")

# Create dose-response curves similar to reference code
cat("\n=== Creating Dose-Response Curves ===\n")

# Extract true probabilities for dose-response curves
true_toxicity <- p_YT_given_I[,1] * (1 - p_YI) + p_YT_given_I[,2] * p_YI
true_efficacy <- p_YE_given_I[,1] * (1 - p_YI) + p_YE_given_I[,2] * p_YI

# Calculate true utilities for all dose levels using true probabilities
true_utility <- sapply(1:length(trial_config$dose_levels), 
                       calculate_utility_from_true_probs, 
                       p_YI = p_YI, 
                       p_YT_given_I = p_YT_given_I, 
                       p_YE_given_I = p_YE_given_I,
                       utility_table = utility_table)

# Create dose-response curves plot
dose_response_plot <- plot_dose_response_curves(
  toxicity_data = true_toxicity,
  efficacy_data = true_efficacy,
  utility_data = true_utility,
  title = "True Dose-Response Curves",
  file_path = "results/plots/dose_response_curves.png"
)
print(dose_response_plot)

# Plot allocation probabilities over time with modern styling
p_alloc_time <- ggplot(results$all_alloc_probs, aes(x = Stage, y = Prob, color = factor(Dose))) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  labs(title = "Allocation Probabilities Over Time", 
       x = "Stage", y = "Allocation Probability", 
       color = "Dose Level") +
  scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#CC79A7")) +
  theme_bw(base_size = 16) +
  theme(panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5),
        axis.line = element_line(color = "black"))
print(p_alloc_time)

# Visualize participant allocation with better formatting
# Count participants per dose level and stage
allocation_summary <- results$all_data %>%
  group_by(d, stage) %>%
  summarise(n_participants = n(), .groups = 'drop') %>%
  mutate(d = factor(d), stage = factor(stage, levels = 1:5, labels = paste("Stage", 1:5)))

# Plot 1: Allocation by dose level and stage with modern styling
p_alloc <- ggplot(allocation_summary, aes(x = d, y = n_participants, fill = stage)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, color = "black") +
  labs(title = "Participant Allocation by Dose Level and Stage", 
       x = "Dose Level", y = "Number of Participants",
       subtitle = paste("Total participants:", sum(allocation_summary$n_participants))) +
  scale_fill_manual(name = "Stage", values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#CC79A7")) +
  theme_bw(base_size = 16) +
  theme(panel.grid = element_blank(),
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 12),
        axis.line = element_line(color = "black"))
print(p_alloc)

# Plot 2: Cumulative allocation over stages
cumulative_summary <- allocation_summary %>%
  group_by(d) %>%
  mutate(cumulative_participants = cumsum(n_participants)) %>%
  ungroup()

p_cumulative <- ggplot(cumulative_summary, aes(x = stage, y = cumulative_participants, color = d, group = d)) +
  geom_line(linewidth = 1.5) +
  geom_point(size = 3) +
  labs(title = "Cumulative Participant Allocation Over Stages", 
       x = "Stage", y = "Cumulative Number of Participants",
       color = "Dose Level",
       subtitle = paste("Final total participants:", sum(allocation_summary$n_participants))) +
  scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#CC79A7")) +
  theme_bw(base_size = 16) +
  theme(panel.grid = element_blank(),
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 12),
        axis.line = element_line(color = "black"))
print(p_cumulative)

# Print summary statistics
cat("\n=== ALLOCATION SUMMARY ===\n")
cat("Total participants:", sum(allocation_summary$n_participants), "\n")
cat("Participants per stage:\n")
stage_totals <- allocation_summary %>%
  group_by(stage) %>%
  summarise(total = sum(n_participants), .groups = 'drop')
for(i in 1:nrow(stage_totals)) {
  cat("  Stage", i, ":", stage_totals$total[i], "participants\n")
}

```

## Method Comparison Analysis

This section creates comparison plots similar to the reference code, showing how different methods or parameter settings would perform.

```{r method_comparison}
# Create example data for method comparison (similar to reference code)
cat("\n=== Creating Method Comparison Plots ===\n")

# Simulate different method performances
methods <- c("Current", "Proposed", "Reference")
scenarios <- c("Scenario 1", "Scenario 2", "Scenario 3")

# OBD Selection Rate Comparison
obd_data <- expand.grid(
  scenario = scenarios,
  method = methods,
  stringsAsFactors = FALSE
)
obd_data$obd_rate <- c(45, 60, 55, 70, 65, 50, 80, 85, 75)

# Create OBD selection plot
p_obd <- plot_method_comparison_bars(
  obd_data,
  x_var = "scenario", y_var = "obd_rate", fill_var = "method",
  title = "OBD Selection Rate Comparison",
  y_label = "OBD Selection (%)",
  limits = c(0, 100),
  file_path = "results/plots/obd_selection_comparison.png"
)
print(p_obd)

# Sample Size Comparison
sample_data <- expand.grid(
  scenario = scenarios,
  method = methods,
  stringsAsFactors = FALSE
)
sample_data$avg_n <- c(25, 20, 30, 22, 18, 28, 18, 15, 25)

# Create sample size plot
p_sample <- plot_method_comparison_bars(
  sample_data,
  x_var = "scenario", y_var = "avg_n", fill_var = "method",
  title = "Average Sample Size Comparison",
  y_label = "Average Sample Size",
  limits = c(0, 35),
  file_path = "results/plots/sample_size_comparison.png"
)
print(p_sample)

# Safety (Overdose) Comparison
safety_data <- expand.grid(
  scenario = scenarios,
  method = methods,
  stringsAsFactors = FALSE
)
safety_data$overdose_pct <- c(15, 10, 20, 12, 8, 18, 8, 5, 15)

# Create safety plot
p_safety <- plot_method_comparison_bars(
  safety_data,
  x_var = "scenario", y_var = "overdose_pct", fill_var = "method",
  title = "Overdose Patient Percentage",
  y_label = "Overdose Pts (%)",
  limits = c(0, 25),
  file_path = "results/plots/safety_comparison.png"
)
print(p_safety)

cat("âœ… Method comparison plots created successfully!\n")
cat("ðŸ“ All plots saved to results/plots/ directory\n")
```

## Multi-Scenario Analysis

This section creates multi-scenario dose-response curves similar to the reference code.

```{r multi_scenario}
# Create multi-scenario analysis
cat("\n=== Creating Multi-Scenario Analysis ===\n")

# Define different scenarios with varying parameters
scenarios_data <- list(
  list(
    toxicity = c(0.1, 0.18, 0.35, 0.40, 0.50),
    efficacy = c(0.35, 0.35, 0.37, 0.39, 0.39),
    utility = c(0.27, 0.23, 0.10, 0.13, 0.17)
  ),
  list(
    toxicity = c(0.05, 0.15, 0.25, 0.35, 0.50),
    efficacy = c(0.10, 0.35, 0.35, 0.38, 0.39),
    utility = c(0.07, 0.22, 0.22, 0.12, 0.06)
  ),
  list(
    toxicity = c(0.02, 0.06, 0.10, 0.20, 0.35),
    efficacy = c(0.05, 0.10, 0.35, 0.35, 0.40),
    utility = c(0.03, 0.07, 0.28, 0.22, 0.13)
  )
)

# Create multi-scenario plot
multi_scenario_plot <- plot_multi_scenario_curves(
  scenarios_data,
  title = "Dose-Response Curves Across Scenarios",
  file_path = "results/plots/multi_scenario_analysis.png"
)
print(multi_scenario_plot)

cat("âœ… Multi-scenario analysis completed!\n")
```

## Calibration Framework

This section demonstrates the calibration framework for optimizing trial parameters.

### Setup Calibration

```{r calibration-setup}
# Use absolute paths for calibration functions
project_root <- "/Users/jz/Development/DoseFinding"

# Source calibration functions
source(file.path(project_root, "src/optimization/poc_calibration.R"))
source(file.path(project_root, "src/optimization/early_termination_calibration.R"))
source(file.path(project_root, "src/utils/calibration_plots.R"))

# Set up output directory for calibration results
calibration_output_dir <- "results/notebook_calibration"
dir.create(calibration_output_dir, showWarnings = FALSE, recursive = TRUE)
```

### PoC Calibration

Calibrate the PoC threshold to achieve target detection rates in null scenarios.

```{r poc-calibration}
# Run PoC calibration with reduced simulations for notebook
cat("Running PoC calibration...\n")
poc_results <- run_quick_calibration(
  target_rate = 0.10,
  n_simulations = 100  # Reduced for notebook
)

# Display results
cat("PoC Calibration Results:\n")
cat("Optimal C_poc =", poc_results$optimal_c_poc, "\n")
cat("Achieved rate =", round(poc_results$optimal_rate, 3), "\n")
cat("Target rate =", poc_results$target_rate, "\n")

# Create calibration curve
poc_plot <- plot_poc_calibration_curve(
  poc_results,
  target_rate = 0.10,
  save_path = file.path(calibration_output_dir, "poc_calibration_curve.png")
)
print(poc_plot)
```

### Early Termination Calibration

Calibrate early termination parameters to achieve target termination rates in unfavorable scenarios.

```{r early-termination-calibration}
# Run early termination calibration with reduced simulations for notebook
cat("Running early termination calibration...\n")
early_term_results <- run_quick_early_termination_calibration(
  target_rate = 0.80,
  n_simulations = 100  # Reduced for notebook
)

# Display results
cat("Early Termination Calibration Results:\n")
cat("Optimal", early_term_results$threshold_type, "=", early_term_results$optimal_threshold, "\n")
cat("Achieved rate =", round(early_term_results$optimal_rate, 3), "\n")
cat("Target rate =", early_term_results$target_rate, "\n")

# Create calibration curve
early_term_plot <- plot_early_termination_curve(
  early_term_results,
  target_rate = 0.80,
  save_path = file.path(calibration_output_dir, "early_termination_calibration_curve.png")
)
print(early_term_plot)
```

### Combined Performance Curves

Visualize both calibration results together.

```{r combined-calibration}
# Create combined calibration data
combined_calibration_data <- list(
  poc_calibration = poc_results,
  termination_calibration = early_term_results
)

# Create combined performance curves
combined_plot <- plot_threshold_performance_curves(
  combined_calibration_data,
  save_path = file.path(calibration_output_dir, "combined_performance_curves.png")
)
print(combined_plot)
```

### Calibration Summary

```{r calibration-summary}
# Create summary table
calibration_summary <- data.frame(
  Parameter = c("C_poc (PoC)", "C_T (Early Termination)", "PoC Target", "PoC Achieved", 
                "Early Termination Target", "Early Termination Achieved"),
  Value = c(
    round(poc_results$optimal_c_poc, 3),
    round(early_term_results$optimal_threshold, 3),
    poc_results$target_rate,
    round(poc_results$optimal_rate, 3),
    early_term_results$target_rate,
    round(early_term_results$optimal_rate, 3)
  )
)

# Display summary table
kable(calibration_summary, caption = "Calibration Summary")
```

## Summary

This notebook demonstrates the Bayesian dose-finding trial simulation with modern, publication-ready visualizations inspired by the reference code. The plots include:

-   **Dose-response curves** with toxicity, efficacy, and utility
-   **Posterior summaries** with modern styling
-   **Allocation analysis** showing participant distribution
-   **Method comparisons** for performance evaluation
-   **Multi-scenario analysis** for parameter sensitivity
-   **Calibration framework** for optimizing trial parameters

All plots use consistent color schemes and professional styling suitable for academic publications and regulatory submissions.

\`\`\`